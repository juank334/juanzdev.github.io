---
layout: post
title: An easy introduction to Machine Learning
published: true
---

## Generalization
When you train a machine learning algorithm we know that the input data is changing the internal values of the model, you can train a machine learing algorithm with you data so well that the model can produce an exact output function that resembles exactly as the training set, this can be risky because your goal is to be able to identify the hidden rule that describes your data, if your model can deduce this hidden rule then the model will be able to behave very well on unseen data, the capability of the model to deduce this hidden rule is what is called generalization, because your model was trained on the training set but was able to learn the rules that describe the data very well, the model generalizes well.

## Overfitting
When you train a machine learning algorithm with small amounts of data or maybe with bad data your model after training will try to describe your training set exactly as it is, this is bad because your model must be prepared to see unseen data on real world problems, in this case your model is so thighted to the training set that at the end is unpractical to use it on new data, a common way to overcome overfitting is to gatther much more data or ease the learned function. 

## Underfitting

## Parameter

## Hyperparameter

## Bias

## Variance





Enter text in [Markdown](http://daringfireball.net/projects/markdown/). Use the toolbar above, or click the **?** button for formatting help.
